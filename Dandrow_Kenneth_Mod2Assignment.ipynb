{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 17s 349us/sample - loss: 0.2828 - accuracy: 0.9184 - val_loss: 0.1525 - val_accuracy: 0.9561\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 16s 321us/sample - loss: 0.1163 - accuracy: 0.9653 - val_loss: 0.1008 - val_accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 15s 309us/sample - loss: 0.0763 - accuracy: 0.9772 - val_loss: 0.0834 - val_accuracy: 0.9756\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 17s 331us/sample - loss: 0.0546 - accuracy: 0.9839 - val_loss: 0.0872 - val_accuracy: 0.9753\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 16s 323us/sample - loss: 0.0399 - accuracy: 0.9881 - val_loss: 0.0835 - val_accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# 512 Neurons\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Prepare the data\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "# Split validation set\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Build the model (2 hidden layers)\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 392us/sample - loss: 0.0741 - accuracy: 0.9778\n",
      "Test accuracy: 0.9778000116348267\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# Evaluate the model / 512 Neurons\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 11s 226us/sample - loss: 0.3161 - accuracy: 0.9119 - val_loss: 0.1754 - val_accuracy: 0.9501\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 10s 209us/sample - loss: 0.1396 - accuracy: 0.9598 - val_loss: 0.1171 - val_accuracy: 0.9675\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 11s 228us/sample - loss: 0.0956 - accuracy: 0.9725 - val_loss: 0.0926 - val_accuracy: 0.9729\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 11s 216us/sample - loss: 0.0715 - accuracy: 0.9791 - val_loss: 0.0944 - val_accuracy: 0.9724\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 11s 225us/sample - loss: 0.0559 - accuracy: 0.9837 - val_loss: 0.0797 - val_accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# 256 Neurons\n",
    "\n",
    "model_256 = keras.Sequential([\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_256.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "history_256 = model_256.fit(x_train, y_train,\n",
    "                            epochs=5,\n",
    "                            batch_size=128,\n",
    "                            validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 135us/sample - loss: 0.0756 - accuracy: 0.9783\n",
      "Test accuracy (256 neurons): 0.9782999753952026\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# 256 Neuron Test\n",
    "\n",
    "test_loss_256, test_acc_256 = model_256.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy (256 neurons): {test_acc_256}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 26s 515us/sample - loss: 0.2521 - accuracy: 0.9268 - val_loss: 0.1386 - val_accuracy: 0.9574\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 23s 451us/sample - loss: 0.0968 - accuracy: 0.9709 - val_loss: 0.0893 - val_accuracy: 0.9747\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 23s 451us/sample - loss: 0.0614 - accuracy: 0.9816 - val_loss: 0.0741 - val_accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 24s 487us/sample - loss: 0.0436 - accuracy: 0.9869 - val_loss: 0.0772 - val_accuracy: 0.9776\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 24s 489us/sample - loss: 0.0310 - accuracy: 0.9905 - val_loss: 0.0721 - val_accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# 1024 Neurons\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Prepare the data\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "# Split validation set\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "model_1024 = keras.Sequential([\n",
    "    layers.Dense(1024, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_1024.compile(optimizer=\"rmsprop\",\n",
    "                   loss=\"sparse_categorical_crossentropy\",\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "history_1024 = model_1024.fit(x_train, y_train,\n",
    "                              epochs=5,\n",
    "                              batch_size=128,\n",
    "                              validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 576us/sample - loss: 0.0646 - accuracy: 0.9811\n",
      "Test accuracy (1024 neurons): 0.9811000227928162\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# 1024 Neuron Test\n",
    "\n",
    "test_loss_1024, test_acc_1024 = model_1024.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy (1024 neurons): {test_acc_1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fcecf8251ce7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-fcecf8251ce7>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    | Model (Hidden Neurons) | Training Accuracy | Validation Accuracy | Test Accuracy |\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Kenneth Dandrow\n",
    "# Accuracy Rate Summary\n",
    "\n",
    "| Model (Hidden Neurons) | Training Accuracy | Validation Accuracy | Test Accuracy |\n",
    "|------------------------|-------------------|---------------------|---------------|\n",
    "| 512 (Default)          | 0.9881            | 0.9756              | 0.9778        |\n",
    "| 256                    | 0.9837            | 0.9756              | 0.9783        |\n",
    "| 1024                   | 0.9905            | 0.9800              | 0.9811        |\n",
    "\n",
    "---\n",
    "\n",
    "# Explanation of Results\n",
    "\n",
    "When comparing the three experiments, all models performed well, achieving over 97% test accuracy. Here's what I observed:\n",
    "\n",
    "- The **256-neuron model** performed slightly lower during training but nearly matched the validation and test accuracy of the 512-neuron model. This suggests it was slightly more efficient but had slightly less capacity to learn complex features.\n",
    "\n",
    "- The **512-neuron model** offered strong overall performance and balanced training time with accuracy. It had the best performance per time invested and is a solid middle ground.\n",
    "\n",
    "- The **1024-neuron model** achieved the highest accuracy across the board. However, it also had **the longest training time**, and the improvement in accuracy was small. This could mean diminishing returns on adding complexity â€” possibly even approaching overfitting if pushed further.\n",
    "\n",
    "Overall, increasing the number of neurons improves accuracy up to a point, but too many neurons can make the model slower and harder to generalize.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
